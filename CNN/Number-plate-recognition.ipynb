{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing openCV\n",
    "import cv2\n",
    "\n",
    "#importing numpy\n",
    "import numpy as np\n",
    "\n",
    "#importing pandas to read the CSV file containing our data\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "#importing keras and sub-libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, MaxPooling2D, Dropout, Conv2D, BatchNormalization, ELU\n",
    "from tensorflow.keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_plate(img):\n",
    "    plate_img = img.copy()\n",
    "    \n",
    "    plate_cascade = cv2.CascadeClassifiers('indian_license_plate.xml')\n",
    "    \n",
    "    plate_rect = plate_cascade.detectMultiScale(plate_img,scaleFactor=1.3,minNeighbors=7)\n",
    "    \n",
    "    for (x,y,w,h) in plate_rect:\n",
    "        a,b = (int(0.02*img.shape[0]),int(0.025*img.shape[1]))\n",
    "        plate = plate_img[y+a:y+h-a,x+b:x+w-b,:]\n",
    "        cv2.rectangle(plate_img,(x,y),(x+w,y+h),(51,51,255),3)\n",
    "        \n",
    "    return plate_img,plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_characters(img):\n",
    "    img1 = cv2.resize(img,(333,75))\n",
    "    img_gray = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n",
    "    _,img_binary = cv2.threshold(img_gray,200,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    img_erode = cv2.erode(img_binary,(3,3))\n",
    "    img_dilate = cv2.dilate(img_erode,(3,3))\n",
    "    \n",
    "    LP_WIDTH = img_dilate.shape[0]\n",
    "    LP_HEIGHT = img_dilate.shape[1]\n",
    "    \n",
    "    img_dilate[0:3,:] = 255\n",
    "    img_dilate[:,0:3] = 255\n",
    "    img_dilate[72:75,:] = 255\n",
    "    img_dilate[:,330:333] = 255\n",
    "    \n",
    "    dimensions = [LP_WIDTH/6,LP_WIDTH/2,LP_HEIGHT/10,2*LP_HEIGHT/3]\n",
    "    \n",
    "    char_list = find_contours(dimensions,img_dilate)\n",
    "    \n",
    "    return char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contours(dimensions,img):\n",
    "    cntrs,_ = cv2.findContours(img.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    lower_width = dimensions[0]\n",
    "    upper_width = dimensions[1]\n",
    "    lower_height = dimensions[2]\n",
    "    upper_height = dimensions[3]\n",
    "    \n",
    "    cntrs = sorted(cntrs, key=cv2.contourArea, reverse=True)[:15]\n",
    "    \n",
    "    x_cntr_list = []\n",
    "    target_contours = []\n",
    "    img_res = []\n",
    "    \n",
    "    for contour in cntrs:\n",
    "        intX,intY,intW,intH = cv2.boundingRect(contour)\n",
    "        \n",
    "        if intWidth > lower_width and intWidth < upper_width and intHeight > lower_height and intHeight < upper_height :\n",
    "            x_cntr_list.append(intX) #stores the x coordinate of the character's contour, to used later for indexing the contours\n",
    "\n",
    "            char_copy = np.zeros((44,24))\n",
    "            #extracting each character using the enclosing rectangle's coordinates.\n",
    "            char = img[intY:intY+intHeight, intX:intX+intWidth]\n",
    "            char = cv2.resize(char, (20, 40))\n",
    "\n",
    "            # Make result formatted for classification: invert colors\n",
    "            char = cv2.subtract(255, char)\n",
    "\n",
    "            # Resize the image to 24x44 with black border\n",
    "            char_copy[2:42, 2:22] = char\n",
    "            char_copy[0:2, :] = 0\n",
    "            char_copy[:, 0:2] = 0\n",
    "            char_copy[42:44, :] = 0\n",
    "            char_copy[:, 22:24] = 0\n",
    "\n",
    "            img_res.append(char_copy) #List that stores the character's binary image (unsorted)\n",
    "            \n",
    "    indices = sorted(range(len(x_cntr_list)),key=lambda k: x_cntr_list[k])\n",
    "    \n",
    "    img_res_copy = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        img_res_copy.append(img_res[idx])\n",
    "    \n",
    "    img_res = np.array(img_res_copy)\n",
    "    \n",
    "    return img_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16,kernel_size=(3,3),input_shape=(28,28,1),activation='relu'))\n",
    "model.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu'))\n",
    "model.add(Conv2D(filters=48,kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128,activation='relu'))\n",
    "model.add(Dense(units=36,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.00001),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 22, 22, 48)        13872     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 11, 11, 48)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 11, 11, 48)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5808)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               743552    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 36)                4644      \n",
      "=================================================================\n",
      "Total params: 766,868\n",
      "Trainable params: 766,868\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 864 images belonging to 36 classes.\n",
      "Found 216 images belonging to 36 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,width_shift_range=0.05,height_shift_range=0.05)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('data/train',target_size=(28,28),batch_size=1,color_mode='grayscale',class_mode='categorical')\n",
    "validation_generator = train_datagen.flow_from_directory('data/val',target_size=(28,28),batch_size=1,color_mode='grayscale',class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "864/864 [==============================] - 19s 22ms/step - loss: 3.5534 - acc: 0.0405 - val_loss: 3.4872 - val_acc: 0.1157\n",
      "Epoch 2/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 3.3986 - acc: 0.1701 - val_loss: 3.2058 - val_acc: 0.4167\n",
      "Epoch 3/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 2.9344 - acc: 0.3611 - val_loss: 2.5096 - val_acc: 0.5880\n",
      "Epoch 4/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 2.0336 - acc: 0.5625 - val_loss: 1.5387 - val_acc: 0.7130\n",
      "Epoch 5/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 1.2894 - acc: 0.6806 - val_loss: 0.9642 - val_acc: 0.8056\n",
      "Epoch 6/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.9895 - acc: 0.7222 - val_loss: 0.7216 - val_acc: 0.8380\n",
      "Epoch 7/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.7729 - acc: 0.7708 - val_loss: 0.6249 - val_acc: 0.8380\n",
      "Epoch 8/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.6870 - acc: 0.7951 - val_loss: 0.5009 - val_acc: 0.8889\n",
      "Epoch 9/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.6057 - acc: 0.8160 - val_loss: 0.4867 - val_acc: 0.8843\n",
      "Epoch 10/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.5336 - acc: 0.8275 - val_loss: 0.3917 - val_acc: 0.9213\n",
      "Epoch 11/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.4588 - acc: 0.8576 - val_loss: 0.3346 - val_acc: 0.9028\n",
      "Epoch 12/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.4141 - acc: 0.8738 - val_loss: 0.3341 - val_acc: 0.8981\n",
      "Epoch 13/100\n",
      "864/864 [==============================] - 19s 22ms/step - loss: 0.3842 - acc: 0.8854 - val_loss: 0.2885 - val_acc: 0.9120\n",
      "Epoch 14/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.3549 - acc: 0.8831 - val_loss: 0.2752 - val_acc: 0.9213\n",
      "Epoch 15/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.3177 - acc: 0.8958 - val_loss: 0.2430 - val_acc: 0.9398\n",
      "Epoch 16/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.3319 - acc: 0.8900 - val_loss: 0.2357 - val_acc: 0.9259\n",
      "Epoch 17/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.2735 - acc: 0.9248 - val_loss: 0.2075 - val_acc: 0.9306\n",
      "Epoch 18/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.2754 - acc: 0.9144 - val_loss: 0.2077 - val_acc: 0.9398\n",
      "Epoch 19/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.2373 - acc: 0.9248 - val_loss: 0.1994 - val_acc: 0.9491\n",
      "Epoch 20/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.2110 - acc: 0.9363 - val_loss: 0.1608 - val_acc: 0.9676\n",
      "Epoch 21/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.2057 - acc: 0.9271 - val_loss: 0.1802 - val_acc: 0.9583\n",
      "Epoch 22/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.1910 - acc: 0.9375 - val_loss: 0.1770 - val_acc: 0.9444\n",
      "Epoch 23/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.2085 - acc: 0.9294 - val_loss: 0.1528 - val_acc: 0.9583\n",
      "Epoch 24/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.1905 - acc: 0.9410 - val_loss: 0.1503 - val_acc: 0.9537\n",
      "Epoch 25/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.2025 - acc: 0.9306 - val_loss: 0.1396 - val_acc: 0.9537\n",
      "Epoch 26/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.1624 - acc: 0.9444 - val_loss: 0.0933 - val_acc: 0.9769\n",
      "Epoch 27/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.1572 - acc: 0.9537 - val_loss: 0.1227 - val_acc: 0.9676\n",
      "Epoch 28/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.1510 - acc: 0.9572 - val_loss: 0.1224 - val_acc: 0.9491\n",
      "Epoch 29/100\n",
      "864/864 [==============================] - 19s 22ms/step - loss: 0.1462 - acc: 0.9525 - val_loss: 0.1189 - val_acc: 0.9676\n",
      "Epoch 30/100\n",
      "864/864 [==============================] - 19s 21ms/step - loss: 0.1507 - acc: 0.9444 - val_loss: 0.0953 - val_acc: 0.9676\n",
      "Epoch 31/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.1380 - acc: 0.9525 - val_loss: 0.1498 - val_acc: 0.9398\n",
      "Epoch 32/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.1458 - acc: 0.9514 - val_loss: 0.1039 - val_acc: 0.9537\n",
      "Epoch 33/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.1307 - acc: 0.9537 - val_loss: 0.1164 - val_acc: 0.9630\n",
      "Epoch 34/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.1060 - acc: 0.9676 - val_loss: 0.1043 - val_acc: 0.9722\n",
      "Epoch 35/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.1338 - acc: 0.9549 - val_loss: 0.0806 - val_acc: 0.9769\n",
      "Epoch 36/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.1214 - acc: 0.9606 - val_loss: 0.1097 - val_acc: 0.9630\n",
      "Epoch 37/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.0994 - acc: 0.9595 - val_loss: 0.0761 - val_acc: 0.9815\n",
      "Epoch 38/100\n",
      "864/864 [==============================] - 18s 21ms/step - loss: 0.1040 - acc: 0.9595 - val_loss: 0.1099 - val_acc: 0.9537\n",
      "Epoch 39/100\n",
      "864/864 [==============================] - 17s 20ms/step - loss: 0.0947 - acc: 0.9699 - val_loss: 0.0970 - val_acc: 0.9491\n",
      "Epoch 40/100\n",
      "864/864 [==============================] - 17s 20ms/step - loss: 0.1054 - acc: 0.9606 - val_loss: 0.0881 - val_acc: 0.9722\n",
      "Epoch 41/100\n",
      "864/864 [==============================] - 18s 20ms/step - loss: 0.0990 - acc: 0.9722 - val_loss: 0.0853 - val_acc: 0.9676\n",
      "Epoch 42/100\n",
      "864/864 [==============================] - 18s 20ms/step - loss: 0.0898 - acc: 0.9641 - val_loss: 0.0516 - val_acc: 0.9954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x195969d0f88>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class stop_training_callback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        if logs.get('val_acc') > 0.99:\n",
    "            self.model.stop_training = True\n",
    "\n",
    "log_dir = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)\n",
    "\n",
    "batch_size = 1\n",
    "callbacks = [tensorboard_callback, stop_training_callback()]\n",
    "model.fit_generator(train_generator,validation_data=validation_generator,epochs=100,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
