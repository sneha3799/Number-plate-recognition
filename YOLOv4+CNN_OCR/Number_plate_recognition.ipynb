{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Number-plate-recognition.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS2l7ls-bmaU"
      },
      "source": [
        "#importing openCV\n",
        "import cv2\n",
        "\n",
        "#importing numpy\n",
        "import numpy as np\n",
        "\n",
        "#importing pandas to read the CSV file containing our data\n",
        "import pandas as pd\n",
        "\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "#importing keras and sub-libraries\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, MaxPooling2D, Dropout, Conv2D, BatchNormalization, ELU\n",
        "from tensorflow.keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wmv06kfbmal"
      },
      "source": [
        "def extract_plate(img):\n",
        "    \n",
        "    height, width, channels = img.shape\n",
        "    \n",
        "    labelsPath = \"./coco.names\"\n",
        "    weightsPath = \"./custom-yolov4-detector_best.weights\"\n",
        "    configPath = \"./custom-yolov4-detector.cfg\"\n",
        "    net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
        "    \n",
        "    plate_img = img.copy()\n",
        "    classes = []\n",
        "    with open(labelsPath, \"r\") as f:\n",
        "        classes = [line.strip() for line in f.readlines()]\n",
        "    layers_names = net.getLayerNames()\n",
        "    output_layers = [layers_names[i[0]-1] for i in net.getUnconnectedOutLayers()]\n",
        "    colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
        "    blob = cv2.dnn.blobFromImage(img, scalefactor=0.00392,size=(320,320), mean=(0,0,0), swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    outputs = net.forward(output_layers)\n",
        "    boxes = []\n",
        "    confs = []\n",
        "    class_ids = []\n",
        "    for output in outputs:\n",
        "        for detect in output:\n",
        "            scores = detect[5:]\n",
        "            #print(scores)\n",
        "            class_id = np.argmax(scores)\n",
        "            conf = scores[class_id]\n",
        "            if conf > 0.8:\n",
        "                center_x = int(detect[0] * width)\n",
        "                center_y = int(detect[1] * height)\n",
        "                w = int(detect[2] * width)\n",
        "                h = int(detect[3] * height)\n",
        "                x = int(center_x - w/2)\n",
        "                y = int(center_y - h / 2)\n",
        "                boxes.append([x, y, w, h])\n",
        "                confs.append(float(conf))\n",
        "                class_ids.append(class_id)\n",
        "                \n",
        "    return boxes, confs, class_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzJw_YuYbman"
      },
      "source": [
        "def draw_labels(boxes, confs, colors, class_ids, classes, img): \n",
        "    indexes = cv2.dnn.NMSBoxes(boxes, confs, 0.5, 0.4)\n",
        "    font = cv2.FONT_HERSHEY_PLAIN\n",
        "    for i in range(len(boxes)):\n",
        "        if i in indexes:\n",
        "            x, y, w, h = boxes[i]\n",
        "            label = str(classes[class_ids[i]])\n",
        "            color = colors[i]\n",
        "            cv2.rectangle(img, (x,y), (x+w, y+h), color, 2)\n",
        "            cv2.putText(img, label, (x, y - 5), font, 1, color, 1)\n",
        "    cv2.imshow(\"Image\", img)\n",
        "    cv2.waitKey()\n",
        "    cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43Fd7_ZNbmao"
      },
      "source": [
        "def segment_characters(img):\n",
        "    img1 = cv2.resize(img,(333,75))\n",
        "    img_gray = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n",
        "    _,img_binary = cv2.threshold(img_gray,200,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "    img_erode = cv2.erode(img_binary,(3,3))\n",
        "    img_dilate = cv2.dilate(img_erode,(3,3))\n",
        "    \n",
        "    LP_WIDTH = img_dilate.shape[0]\n",
        "    LP_HEIGHT = img_dilate.shape[1]\n",
        "    \n",
        "    img_dilate[0:3,:] = 255\n",
        "    img_dilate[:,0:3] = 255\n",
        "    img_dilate[72:75,:] = 255\n",
        "    img_dilate[:,330:333] = 255\n",
        "    \n",
        "    dimensions = [LP_WIDTH/6,LP_WIDTH/2,LP_HEIGHT/10,2*LP_HEIGHT/3]\n",
        "    \n",
        "    char_list = find_contours(dimensions,img_dilate)\n",
        "    \n",
        "    return char_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi92auTibmap"
      },
      "source": [
        "def find_contours(dimensions,img):\n",
        "    cntrs,_ = cv2.findContours(img.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    \n",
        "    lower_width = dimensions[0]\n",
        "    upper_width = dimensions[1]\n",
        "    lower_height = dimensions[2]\n",
        "    upper_height = dimensions[3]\n",
        "    \n",
        "    cntrs = sorted(cntrs, key=cv2.contourArea, reverse=True)[:15]\n",
        "    \n",
        "    x_cntr_list = []\n",
        "    target_contours = []\n",
        "    img_res = []\n",
        "    \n",
        "    for contour in cntrs:\n",
        "        intX,intY,intWidth,intHeight = cv2.boundingRect(contour)\n",
        "        \n",
        "        if intWidth > lower_width and intWidth < upper_width and intHeight > lower_height and intHeight < upper_height :\n",
        "            x_cntr_list.append(intX) #stores the x coordinate of the character's contour, to used later for indexing the contours\n",
        "\n",
        "            char_copy = np.zeros((44,24))\n",
        "            #extracting each character using the enclosing rectangle's coordinates.\n",
        "            char = img[intY:intY+intHeight, intX:intX+intWidth]\n",
        "            char = cv2.resize(char, (20, 40))\n",
        "\n",
        "            # Make result formatted for classification: invert colors\n",
        "            char = cv2.subtract(255, char)\n",
        "\n",
        "            # Resize the image to 24x44 with black border\n",
        "            char_copy[2:42, 2:22] = char\n",
        "            char_copy[0:2, :] = 0\n",
        "            char_copy[:, 0:2] = 0\n",
        "            char_copy[42:44, :] = 0\n",
        "            char_copy[:, 22:24] = 0\n",
        "\n",
        "            #Reshaping \n",
        "            char_copy = cv2.resize(char_copy,(28,28))\n",
        "            img_res.append(char_copy) #List that stores the character's binary image (unsorted)\n",
        "            \n",
        "    indices = sorted(range(len(x_cntr_list)),key=lambda k: x_cntr_list[k])\n",
        "    \n",
        "    img_res_copy = []\n",
        "    \n",
        "    for idx in indices:\n",
        "        img_res_copy.append(img_res[idx])\n",
        "    \n",
        "    img_res = np.array(img_res_copy)\n",
        "    \n",
        "    return img_res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "AlRbb-w7bmaq"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(filters=16,kernel_size=(3,3),input_shape=(28,28,1),activation='relu'))\n",
        "model.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu'))\n",
        "model.add(Conv2D(filters=48,kernel_size=(3,3),activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=128,activation='relu'))\n",
        "model.add(Dense(units=36,activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.00001),metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPpLgqqdbmar",
        "outputId": "452cc192-7d98-4dfa-ff94-b4a1a23439e8"
      },
      "source": [
        "'''from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,width_shift_range=0.05,height_shift_range=0.05)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory('data/train',target_size=(28,28),batch_size=1,color_mode='grayscale',class_mode='categorical')\n",
        "validation_generator = train_datagen.flow_from_directory('data/val',target_size=(28,28),batch_size=1,color_mode='grayscale',class_mode='categorical')'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 864 images belonging to 36 classes.\n",
            "Found 216 images belonging to 36 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PFkWY-u7bmau",
        "outputId": "6f4c2bf7-0a3a-439a-a162-5248c997fdb1"
      },
      "source": [
        "'''class stop_training_callback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self,epoch,logs={}):\n",
        "        if logs.get('val_acc') > 0.99:\n",
        "            self.model.stop_training = True\n",
        "\n",
        "log_dir = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)\n",
        "\n",
        "batch_size = 1\n",
        "callbacks = [tensorboard_callback, stop_training_callback()]\n",
        "model.fit_generator(train_generator,validation_data=validation_generator,epochs=100)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "864/864 [==============================] - 26s 30ms/step - loss: 3.5499 - acc: 0.0521 - val_loss: 3.4692 - val_acc: 0.2176\n",
            "Epoch 2/100\n",
            "864/864 [==============================] - 14s 16ms/step - loss: 3.3723 - acc: 0.2002 - val_loss: 3.1400 - val_acc: 0.4676\n",
            "Epoch 3/100\n",
            "864/864 [==============================] - 14s 16ms/step - loss: 2.8458 - acc: 0.4051 - val_loss: 2.3008 - val_acc: 0.6481\n",
            "Epoch 4/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 1.8781 - acc: 0.6042 - val_loss: 1.3542 - val_acc: 0.7593\n",
            "Epoch 5/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 1.2064 - acc: 0.6956 - val_loss: 0.8930 - val_acc: 0.8148\n",
            "Epoch 6/100\n",
            "864/864 [==============================] - 13s 14ms/step - loss: 0.8215 - acc: 0.7755 - val_loss: 0.6725 - val_acc: 0.8241\n",
            "Epoch 7/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.7127 - acc: 0.7905 - val_loss: 0.5456 - val_acc: 0.8704\n",
            "Epoch 8/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.6453 - acc: 0.8113 - val_loss: 0.4817 - val_acc: 0.8981\n",
            "Epoch 9/100\n",
            "864/864 [==============================] - 13s 14ms/step - loss: 0.5422 - acc: 0.8299 - val_loss: 0.3978 - val_acc: 0.9028\n",
            "Epoch 10/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.5003 - acc: 0.8449 - val_loss: 0.3501 - val_acc: 0.9028\n",
            "Epoch 11/100\n",
            "864/864 [==============================] - 13s 14ms/step - loss: 0.4306 - acc: 0.8681 - val_loss: 0.3365 - val_acc: 0.8981\n",
            "Epoch 12/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.3992 - acc: 0.8727 - val_loss: 0.2449 - val_acc: 0.9491\n",
            "Epoch 13/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.4120 - acc: 0.8669 - val_loss: 0.3159 - val_acc: 0.9213\n",
            "Epoch 14/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.3517 - acc: 0.8808 - val_loss: 0.2745 - val_acc: 0.9306\n",
            "Epoch 15/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.3331 - acc: 0.8935 - val_loss: 0.2556 - val_acc: 0.9398\n",
            "Epoch 16/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.3028 - acc: 0.9051 - val_loss: 0.2132 - val_acc: 0.9398\n",
            "Epoch 17/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.2612 - acc: 0.9201 - val_loss: 0.2517 - val_acc: 0.9259\n",
            "Epoch 18/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.2250 - acc: 0.9294 - val_loss: 0.2346 - val_acc: 0.9167\n",
            "Epoch 19/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.2368 - acc: 0.9282 - val_loss: 0.2364 - val_acc: 0.9259\n",
            "Epoch 20/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.2103 - acc: 0.9294 - val_loss: 0.1792 - val_acc: 0.9259\n",
            "Epoch 21/100\n",
            "864/864 [==============================] - 14s 16ms/step - loss: 0.2127 - acc: 0.9259 - val_loss: 0.1467 - val_acc: 0.9491\n",
            "Epoch 22/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.1914 - acc: 0.9433 - val_loss: 0.1659 - val_acc: 0.9537\n",
            "Epoch 23/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.2063 - acc: 0.9294 - val_loss: 0.1589 - val_acc: 0.9352\n",
            "Epoch 24/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.1967 - acc: 0.9340 - val_loss: 0.1214 - val_acc: 0.9676\n",
            "Epoch 25/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.1767 - acc: 0.9421 - val_loss: 0.1716 - val_acc: 0.9306\n",
            "Epoch 26/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.1629 - acc: 0.9433 - val_loss: 0.1317 - val_acc: 0.9537\n",
            "Epoch 27/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.1749 - acc: 0.9398 - val_loss: 0.1618 - val_acc: 0.9398\n",
            "Epoch 28/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.1502 - acc: 0.9525 - val_loss: 0.1573 - val_acc: 0.9537\n",
            "Epoch 29/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.1670 - acc: 0.9433 - val_loss: 0.1398 - val_acc: 0.9491\n",
            "Epoch 30/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.1241 - acc: 0.9525 - val_loss: 0.1194 - val_acc: 0.9676\n",
            "Epoch 31/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.1579 - acc: 0.9549 - val_loss: 0.1101 - val_acc: 0.9676\n",
            "Epoch 32/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.1245 - acc: 0.9583 - val_loss: 0.1000 - val_acc: 0.9722\n",
            "Epoch 33/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.1352 - acc: 0.9479 - val_loss: 0.1164 - val_acc: 0.9583\n",
            "Epoch 34/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.1060 - acc: 0.9583 - val_loss: 0.0934 - val_acc: 0.9630\n",
            "Epoch 35/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.1110 - acc: 0.9630 - val_loss: 0.1090 - val_acc: 0.9630\n",
            "Epoch 36/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.1214 - acc: 0.9606 - val_loss: 0.1112 - val_acc: 0.9583\n",
            "Epoch 37/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.1151 - acc: 0.9630 - val_loss: 0.0951 - val_acc: 0.9630\n",
            "Epoch 38/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.0872 - acc: 0.9711 - val_loss: 0.0770 - val_acc: 0.9815\n",
            "Epoch 39/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0977 - acc: 0.9630 - val_loss: 0.1221 - val_acc: 0.9583\n",
            "Epoch 40/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.1045 - acc: 0.9618 - val_loss: 0.0984 - val_acc: 0.9722\n",
            "Epoch 41/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.1028 - acc: 0.9641 - val_loss: 0.0900 - val_acc: 0.9630\n",
            "Epoch 42/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0861 - acc: 0.9676 - val_loss: 0.0812 - val_acc: 0.9769\n",
            "Epoch 43/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.1105 - acc: 0.9560 - val_loss: 0.0696 - val_acc: 0.9769\n",
            "Epoch 44/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.0926 - acc: 0.9722 - val_loss: 0.1071 - val_acc: 0.9583\n",
            "Epoch 45/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.0987 - acc: 0.9711 - val_loss: 0.0622 - val_acc: 0.9769\n",
            "Epoch 46/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.1063 - acc: 0.9560 - val_loss: 0.0703 - val_acc: 0.9769\n",
            "Epoch 47/100\n",
            "864/864 [==============================] - 13s 14ms/step - loss: 0.0810 - acc: 0.9734 - val_loss: 0.0506 - val_acc: 0.9907\n",
            "Epoch 48/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.0817 - acc: 0.9711 - val_loss: 0.0719 - val_acc: 0.9676\n",
            "Epoch 49/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.0813 - acc: 0.9676 - val_loss: 0.0803 - val_acc: 0.9769\n",
            "Epoch 50/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.0821 - acc: 0.9734 - val_loss: 0.0415 - val_acc: 0.9907\n",
            "Epoch 51/100\n",
            "864/864 [==============================] - 13s 14ms/step - loss: 0.0825 - acc: 0.9699 - val_loss: 0.0760 - val_acc: 0.9769\n",
            "Epoch 52/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.0618 - acc: 0.9780 - val_loss: 0.0654 - val_acc: 0.9722\n",
            "Epoch 53/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.0580 - acc: 0.9815 - val_loss: 0.0461 - val_acc: 0.9861\n",
            "Epoch 54/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.0704 - acc: 0.9711 - val_loss: 0.0519 - val_acc: 0.9861\n",
            "Epoch 55/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.0777 - acc: 0.9722 - val_loss: 0.0426 - val_acc: 0.9861\n",
            "Epoch 56/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0656 - acc: 0.9780 - val_loss: 0.0589 - val_acc: 0.9861\n",
            "Epoch 57/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.0756 - acc: 0.9688 - val_loss: 0.0786 - val_acc: 0.9676\n",
            "Epoch 58/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.0794 - acc: 0.9722 - val_loss: 0.0487 - val_acc: 0.9861\n",
            "Epoch 59/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.0579 - acc: 0.9769 - val_loss: 0.0586 - val_acc: 0.9722\n",
            "Epoch 60/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.0625 - acc: 0.9769 - val_loss: 0.0658 - val_acc: 0.9722\n",
            "Epoch 61/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.0668 - acc: 0.9745 - val_loss: 0.0670 - val_acc: 0.9769\n",
            "Epoch 62/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0655 - acc: 0.9699 - val_loss: 0.0496 - val_acc: 0.9769\n",
            "Epoch 63/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0509 - acc: 0.9884 - val_loss: 0.0408 - val_acc: 0.9907\n",
            "Epoch 64/100\n",
            "864/864 [==============================] - 13s 15ms/step - loss: 0.0711 - acc: 0.9769 - val_loss: 0.0595 - val_acc: 0.9722\n",
            "Epoch 65/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0679 - acc: 0.9745 - val_loss: 0.0453 - val_acc: 0.9861\n",
            "Epoch 66/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0511 - acc: 0.9769 - val_loss: 0.0494 - val_acc: 0.9769\n",
            "Epoch 67/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0588 - acc: 0.9780 - val_loss: 0.0433 - val_acc: 0.9861\n",
            "Epoch 68/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0511 - acc: 0.9873 - val_loss: 0.0516 - val_acc: 0.9861\n",
            "Epoch 69/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0434 - acc: 0.9838 - val_loss: 0.0614 - val_acc: 0.9769\n",
            "Epoch 70/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0637 - acc: 0.9792 - val_loss: 0.0452 - val_acc: 0.9861\n",
            "Epoch 71/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0464 - acc: 0.9850 - val_loss: 0.0406 - val_acc: 0.9861\n",
            "Epoch 72/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0598 - acc: 0.9745 - val_loss: 0.0522 - val_acc: 0.9722\n",
            "Epoch 73/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0552 - acc: 0.9826 - val_loss: 0.0319 - val_acc: 0.9907\n",
            "Epoch 74/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0558 - acc: 0.9769 - val_loss: 0.0510 - val_acc: 0.9769\n",
            "Epoch 75/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0546 - acc: 0.9780 - val_loss: 0.0359 - val_acc: 0.9907\n",
            "Epoch 76/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0417 - acc: 0.9826 - val_loss: 0.0361 - val_acc: 0.9954\n",
            "Epoch 77/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0408 - acc: 0.9826 - val_loss: 0.0302 - val_acc: 0.9954\n",
            "Epoch 78/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0444 - acc: 0.9803 - val_loss: 0.0428 - val_acc: 0.9861\n",
            "Epoch 79/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0515 - acc: 0.9815 - val_loss: 0.0284 - val_acc: 0.9907\n",
            "Epoch 80/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0486 - acc: 0.9803 - val_loss: 0.0419 - val_acc: 0.9861\n",
            "Epoch 81/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0576 - acc: 0.9803 - val_loss: 0.0271 - val_acc: 0.9907\n",
            "Epoch 82/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0537 - acc: 0.9803 - val_loss: 0.0378 - val_acc: 0.9861\n",
            "Epoch 83/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0381 - acc: 0.9861 - val_loss: 0.0452 - val_acc: 0.9815\n",
            "Epoch 84/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0540 - acc: 0.9803 - val_loss: 0.0621 - val_acc: 0.9815\n",
            "Epoch 85/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0393 - acc: 0.9907 - val_loss: 0.0255 - val_acc: 0.9954\n",
            "Epoch 86/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0473 - acc: 0.9803 - val_loss: 0.0217 - val_acc: 0.9954\n",
            "Epoch 87/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0447 - acc: 0.9861 - val_loss: 0.0336 - val_acc: 0.9954\n",
            "Epoch 88/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0310 - acc: 0.9861 - val_loss: 0.0309 - val_acc: 0.9907\n",
            "Epoch 89/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0452 - acc: 0.9826 - val_loss: 0.0526 - val_acc: 0.9676\n",
            "Epoch 90/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0346 - acc: 0.9850 - val_loss: 0.0372 - val_acc: 0.9954\n",
            "Epoch 91/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0427 - acc: 0.9826 - val_loss: 0.0227 - val_acc: 0.9954\n",
            "Epoch 92/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0609 - acc: 0.9757 - val_loss: 0.0472 - val_acc: 0.9907\n",
            "Epoch 93/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0443 - acc: 0.9769 - val_loss: 0.0504 - val_acc: 0.9815\n",
            "Epoch 94/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0429 - acc: 0.9838 - val_loss: 0.0392 - val_acc: 0.9815\n",
            "Epoch 95/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0488 - acc: 0.9815 - val_loss: 0.0357 - val_acc: 0.9907\n",
            "Epoch 96/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0378 - acc: 0.9850 - val_loss: 0.0250 - val_acc: 0.9954\n",
            "Epoch 97/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0453 - acc: 0.9838 - val_loss: 0.0278 - val_acc: 0.9954\n",
            "Epoch 98/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0391 - acc: 0.9850 - val_loss: 0.0282 - val_acc: 0.9907\n",
            "Epoch 99/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0528 - acc: 0.9826 - val_loss: 0.0249 - val_acc: 0.9907\n",
            "Epoch 100/100\n",
            "864/864 [==============================] - 12s 14ms/step - loss: 0.0323 - acc: 0.9873 - val_loss: 0.0258 - val_acc: 0.9907\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x1a18052d400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PxMiiHEobmau"
      },
      "source": [
        "model.save(\"OCR_Model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1m79E0Pmbmav"
      },
      "source": [
        "def extractLPNumber(img):\n",
        "    classifier = load_model(\"OCR_Model\")\n",
        "    alphabets = list('0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
        "    result = extract_plate(img)\n",
        "    lp_numbers = []\n",
        "    for dim,_,cls in np.array(result).transpose():\n",
        "        if cls == 0:\n",
        "            x,y,w,h = dim\n",
        "            segment = img[y:y+h,x:x+w]\n",
        "            characters = segment_characters(segment)\n",
        "            if len(characters)>0:\n",
        "                string = ''\n",
        "                pred = classifier.predict(characters.reshape((-1,28,28,1)))\n",
        "                for output in pred:\n",
        "                    #Appending the character\n",
        "                    string += alphabets[np.argmax(output)]\n",
        "                lp_numbers.append(string)\n",
        "                \n",
        "    return lp_numbers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBAp57aebmav"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Dp3CdNhDbmaw",
        "outputId": "9342ebe2-aa43-42de-ec3c-610a4810521e"
      },
      "source": [
        "img = cv2.imread(\"img.jpg\")\n",
        "colors = np.random.uniform(0, 255, size=(3, 3))\n",
        "boxes, confs, class_ids = extract_plate(img)\n",
        "draw_labels(boxes,confs,colors,class_ids,[\"Number Plate\",\"Car\"],img)\n",
        "img = cv2.imread(\"img.jpg\")\n",
        "extractLPNumber(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000018C1D892040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<ipython-input-135-f32b6c3287b7>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  for dim,_,cls in np.array(result).transpose():\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1KAP755', 'KA757']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgKn-bMQbmax"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}